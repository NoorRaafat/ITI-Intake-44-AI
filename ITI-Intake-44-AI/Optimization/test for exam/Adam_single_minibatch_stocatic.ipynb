{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b14123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_sv_stoc(X,y,learining_rate,beta1,beta2,epsloin,n_iteration):\n",
    "    m=len(X)\n",
    "    \n",
    "    theta0, theta1 = 0, 0\n",
    "    \n",
    "    all_loss ,all_theta0,all_theta1,hypothesis_history= [],[],[],[]\n",
    "    \n",
    "    v_t0 = 0\n",
    "    v_t1 = 0\n",
    "    #m=momentum\n",
    "    m_t0=0\n",
    "    m_t1=0\n",
    "    t=0\n",
    "   \n",
    "    for i in range(n_iteration):\n",
    "        \n",
    "        #step 2 : h0(x)\n",
    "        y_hat=theta0+theta1*X\n",
    "        hypothesis_history.append(y_hat)\n",
    "        print(f\"****************** Iteration {i} ********************\\n \")\n",
    "        print(f\"\\n h(x):{y_hat}\")\n",
    "      \n",
    "        error_vector=y_hat-y\n",
    "        \n",
    "         #step3 mean square error #mse=loss(j)=error\n",
    "        loss = np.sum(error_vector**2) / (2 * m)\n",
    "        all_loss.append(loss)\n",
    "        all_theta0.append(theta0)\n",
    "        all_theta1.append(theta1)\n",
    "        \n",
    "        # Gradient thetas(1/m)\n",
    "        #gradient_theta_0 = (1 / m) * np.sum(error_vector)\n",
    "        #gradient_theta_1 = (1 / m) * np.dot(error_vector, x)\n",
    "        g_theta0 = sum(y_hat - y) / m\n",
    "        g_theta1 = sum((y_hat - y) * X) / m\n",
    "        \n",
    "        \n",
    "        # Stopping using gradient\n",
    "        if np.linalg.norm([g_theta0, g_theta1]) < 0.001:\n",
    "            break\n",
    "             # Stopping using loss\n",
    "        if (i > 0) and abs(all_loss[i-1] - all_loss[i]) < 0.001:\n",
    "            break\n",
    "            #updating \n",
    "            \n",
    "        t=t+1\n",
    "        m_t0= beta1 * m_t0 + (1 - beta1 ) * g_theta0\n",
    "        m_t1= beta1 * m_t1 + (1 - beta1 ) * g_theta1\n",
    "        \n",
    "        v_t0= beta2 * v_t0 + (1 - beta2 ) * g_theta0**2\n",
    "        v_t1= beta2 * v_t1 + (1 - beta2 ) * g_theta1**2\n",
    "        \n",
    "        #bias correction\n",
    "        m_t0_hat=m_t0 / (1 - beta1**t)\n",
    "        m_t1_hat=m_t1 / (1 - beta1**t)\n",
    "        \n",
    "        v_t0_hat=v_t0 / (1 - beta2**t)\n",
    "        v_t1_hat=v_t1 / (1 - beta2**t)\n",
    "        \n",
    "            \n",
    "      \n",
    "        theta0=theta0 - (learining_rate /(np.sqrt(v_t0_hat) + epsloin)) * m_t0_hat\n",
    "        theta1=theta1- (learining_rate /(np.sqrt(v_t1_hat) + epsloin)) * m_t1_hat\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    print(\"\\n******* Training Report *******\")\n",
    "    print(f\"Gradient Descent converged after {i} epochs\")\n",
    "    print(\"theta_0_Opt :\",theta0)\n",
    "    print(\"theta_1_Opt :\",theta1)\n",
    "    print(\"Cost = \",all_loss[-1])\n",
    "    print(f\"Y_Hat: \\n{y_hat}\\n\")\n",
    "    print(f\"Y: \\n{y}\")\n",
    "    \n",
    "    \n",
    "    return all_theta0,all_theta1,all_loss,hypothesis_history,theta0,theta1,y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam_mv_mini_batch(X, Y, learning_rate, max_iters, batch_size, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m = len(Y)\n",
    "    cost = []\n",
    "    theta_h = []\n",
    "    hypothesis_h = []\n",
    "\n",
    "    # Initialize parameters\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "    m_theta = np.zeros_like(theta)\n",
    "    v_theta = np.zeros_like(theta)\n",
    "    t = 0\n",
    "\n",
    "    num_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        for j in range(0, m, batch_size):\n",
    "            \n",
    "            X_batch = X[j:j+batch_size]\n",
    "            Y_batch = Y[j:j+batch_size]\n",
    "            \n",
    "            theta_h.append(theta.flatten().tolist())\n",
    "            hypothesis = X_batch.dot(theta)\n",
    "            hypothesis_h.append(hypothesis)\n",
    "            error = hypothesis - Y_batch\n",
    "            loss = np.mean(np.square(error)) / 2\n",
    "            cost.append(loss)\n",
    "            gradient = (1 / len(X_batch)) * X_batch.T.dot(error)\n",
    "\n",
    "            Gradient_Norm = np.linalg.norm(gradient)\n",
    "\n",
    "            if Gradient_Norm < 0.001:\n",
    "                break\n",
    "\n",
    "            if i > 0 and abs(cost[-int(num_batches  + 1)] - cost[-1]) < 0.001:\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "            m_theta = beta1 * m_theta + (1 - beta1) * gradient\n",
    "            v_theta = beta2 * v_theta + (1 - beta2) * (gradient ** 2)\n",
    "\n",
    "            m_hat_theta = m_theta / (1 - beta1 ** t)\n",
    "            v_hat_theta = v_theta / (1 - beta2 ** t)\n",
    "\n",
    "            theta -= (learning_rate / (np.sqrt(v_hat_theta) + epsilon)) * m_hat_theta\n",
    "\n",
    "        print(f\"Iteration {i + 1} - Loss: {loss}\")\n",
    "\n",
    "    print(\"\\n******* Training Report *******\")\n",
    "    print(f\"Adam_mv converged after {i + 1} iterations\")\n",
    "    print(\"Optimized parameters:\")\n",
    "    print(theta)\n",
    "    print(\"Final Cost:\", cost[-1])\n",
    "    y_final = X @ theta\n",
    "    return theta, loss, cost,y_final, hypothesis_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80968510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_stochastic_gradient_descent(X, y, alpha, beta1, beta2, epsilon, epoch, convergence_rate=0.001, norm_value_check=0.001):\n",
    "    np.random.seed(101)\n",
    "    alldata = np.column_stack((X, y))\n",
    "    alldata = shuffle(alldata)\n",
    "    X = alldata[:, :-1]\n",
    "    y = alldata[:, -1].reshape(-1, 1)\n",
    "    m, n = X.shape\n",
    "    \n",
    "    theta = np.zeros((n, 1))\n",
    "    losses = []\n",
    "    theta_all = []\n",
    "    \n",
    "    vt = np.zeros_like(theta)\n",
    "    mt = np.zeros_like(theta)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        for j in range(m):\n",
    "            # Append theta first\n",
    "            theta_all.append(theta.copy())\n",
    "\n",
    "            # Predict\n",
    "            h = X[j] @ theta\n",
    "\n",
    "            # Calculate J & Error Vector\n",
    "            error_vector = h - y[j]\n",
    "            J = np.sum(error_vector**2) / 2\n",
    "            losses.append(J)\n",
    "\n",
    "            # Gradient\n",
    "            gradient = X[j].reshape(-1, 1) * error_vector\n",
    "            grad_norm = np.linalg.norm(gradient)\n",
    "\n",
    "            # Convergence check\n",
    "            if (grad_norm < norm_value_check):\n",
    "                print(\"Converged!\")\n",
    "                return theta, losses, theta_all\n",
    "\n",
    "            # Momentum equations\n",
    "            mt = beta1 * mt + (1 - beta1) * gradient\n",
    "\n",
    "            # RMS equations\n",
    "            vt = beta2 * vt + (1 - beta2) * (gradient**2)\n",
    "\n",
    "            # Bias correction\n",
    "            mt_hat = mt / (1 - beta1**(i + 1))\n",
    "            vt_hat = vt / (1 - beta2**(i + 1))\n",
    "\n",
    "            # Adam update theta\n",
    "            theta = theta - alpha * mt_hat / (np.sqrt(vt_hat) + epsilon)\n",
    "\n",
    "        if ((i > 1) and (abs(losses[- 1] - losses[-(m+1)]) < convergence_rate)):\n",
    "            break\n",
    "            \n",
    "        # Print information for each epoch\n",
    "        print(f\"****************** Epoch {i} ********************\")\n",
    "        print(f\"h(x): {h.flatten()}\")\n",
    "        print(f\"Error Vector: {error_vector.flatten()}\")\n",
    "        print(f\"J = {J}\")\n",
    "        print(f\"Gradient Vector:\\n{gradient}\")\n",
    "        print(f\"Gradient Vector Norm: {grad_norm}\")\n",
    "        print(f\"Theta_new : {theta.flatten()}\\n\")\n",
    "\n",
    "    print(\"Did not converge within the specified number of epochs.\")\n",
    "    return theta, losses, theta_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
